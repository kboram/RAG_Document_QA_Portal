from django.shortcuts import render, redirect, get_object_or_404
from django.db.models import Count, Max, Q
from django.db.models.functions import TruncDate

from .models import UploadedDocument, DocumentChunk, QuestionHistory
from .forms import DocumentUploadForm

from rank_bm25 import BM25Okapi
import numpy as np

# ìœ í‹¸ í•¨ìˆ˜ë“¤
from chatbot.utils.document_loader import extract_text_from_file
from chatbot.utils.chunking import split_into_chunks
from chatbot.utils.vector_store import get_embedding_model
from chatbot.utils.llm_client import generate_answer_with_context
from chatbot.utils.summarizer import summarize_document_chunks
from chatbot.utils.rag_pipeline import hybrid_chunk_search


# ------------------------------------------------
# ë©”ì¸ í™ˆ í™”ë©´
# ------------------------------------------------
def home(request):
    """
    ë©”ì¸ í˜ì´ì§€
    - ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ì„ ë³´ì—¬ì¤Œ
    - ê²€ìƒ‰(q)ìœ¼ë¡œ ì œëª© + ë‚´ìš© ê²€ìƒ‰
    - ë¬¸ì„œë³„ ì§ˆë¬¸ ê°œìˆ˜ / ë§ˆì§€ë§‰ ì§ˆë¬¸ ì‹œê° ê°„ë‹¨ í†µê³„ í¬í•¨
    """

    # ğŸ”¹ 1) ê²€ìƒ‰ì–´(q) ê°€ì ¸ì˜¤ê¸°
    q = request.GET.get("q", "").strip()

    # ğŸ”¹ 2) ê¸°ë³¸ ì¿¼ë¦¬ì…‹
    base_qs = UploadedDocument.objects.all()

    # ğŸ”¹ 3) ê²€ìƒ‰ì–´ê°€ ìˆìœ¼ë©´ ì œëª© + ë‚´ìš©ìœ¼ë¡œ í•„í„°
    if q:
        base_qs = base_qs.filter(
            Q(title__icontains=q) |
            Q(content__icontains=q)
        )

    # ğŸ”¹ 4) ë¬¸ì„œë³„ ê°„ë‹¨ í†µê³„ annotate
    docs = (
        base_qs
        .annotate(
            question_count=Count("questions", distinct=True),
            last_question_at=Max("questions__created_at"),
        )
        .order_by("-uploaded_at")
    )

    context = {
        "documents": docs,
        "search_query": q,
        "total_docs": base_qs.count(),   # ê²€ìƒ‰ í›„ ê¸°ì¤€ ê°œìˆ˜
    }
    return render(request, "chatbot/home.html", context)

# ------------------------------------------------
# ë¬¸ì„œ ì—…ë¡œë“œ
# ------------------------------------------------
def upload_document(request):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ ë·°
    1) ì‚¬ìš©ìê°€ PDF/TXT íŒŒì¼ê³¼ ì œëª©ì„ ì—…ë¡œë“œí•˜ë©´
    2) UploadedDocument ë¡œ ì €ì¥
    3) íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì„œ content í•„ë“œì— ì €ì¥
    4) í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ DocumentChunk ë¡œ ì €ì¥
    """
    if request.method == "POST":
        form = DocumentUploadForm(request.POST, request.FILES)
        if form.is_valid():
            # 1) ë¬¸ì„œ ì •ë³´ + íŒŒì¼ ì €ì¥
            doc = form.save()

            # 2) íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì™€ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            file_path = doc.file.path
            full_text = extract_text_from_file(file_path)

            # 3) ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ content ì— ì €ì¥
            doc.content = full_text or ""
            doc.save()

            # 4) ê¸°ì¡´ ì²­í¬ê°€ ìˆìœ¼ë©´ ì‚­ì œ (ê°™ì€ ë¬¸ì„œ ì¬ì—…ë¡œë“œ ëŒ€ë¹„)
            doc.chunks.all().delete()

            # 5) í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„ë¦¬
            chunks = split_into_chunks(doc.content, max_length=400)

            # 6) ì²­í¬ë“¤ì„ DocumentChunkë¡œ ì €ì¥
            for idx, chunk_text in enumerate(chunks):
                DocumentChunk.objects.create(
                    document=doc,
                    chunk_index=idx,
                    content=chunk_text,
                )

            # 7) ë¬¸ì„œ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
            return redirect("document_detail", pk=doc.pk)
    else:
        form = DocumentUploadForm()

    return render(request, "chatbot/upload.html", {"form": form})


# ------------------------------------------------
# ë¬¸ì„œ ìƒì„¸ (ìš”ì•½ + ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ + íˆìŠ¤í† ë¦¬)
# ------------------------------------------------
def document_detail(request, pk):
    """
    ë¬¸ì„œ ìƒì„¸ í˜ì´ì§€:
    - GET  : ë¬¸ì„œ ë‚´ìš© + ì´ì „ ì§ˆë¬¸ ê¸°ë¡ ë³´ì—¬ì¤Œ
    - POST : mode ê°’ì— ë”°ë¼ ë™ì‘ ë¶„ê¸°
        * mode == 'summary' â†’ ë¬¸ì„œ ìš”ì•½ (GPT ì‚¬ìš©)
        * mode == 'qa'      â†’ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ (RAG + GPT)
        * mode == 'reset'   â†’ ìš”ì•½/ë‹µë³€/ê²€ìƒ‰ê²°ê³¼ ì´ˆê¸°í™”
    """
    # 1) ë¬¸ì„œ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
    doc = get_object_or_404(UploadedDocument, pk=pk)

    # 2) ê¸°ë³¸ ê°’ ì´ˆê¸°í™”
    summary_text = None          # ë¬¸ì„œ ìš”ì•½ ê²°ê³¼
    question = None              # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸
    llm_answer = None            # GPTê°€ ìƒì„±í•œ ë‹µë³€
    search_results = []          # RAG ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    confidence = None            # ì‹ ë¢°ë„ ì ìˆ˜ (%)

    # 3) ê¸°ì¡´ ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ (í•­ìƒ í…œí”Œë¦¿ì— ë³´ë‚´ê¸°)
    history = QuestionHistory.objects.filter(document=doc).order_by("-created_at")

    if request.method == "POST":
        mode = request.POST.get("mode", "qa")  # ê¸°ë³¸ê°’ 'qa'

        # ---------- (A) ë¬¸ì„œ ìš”ì•½ ëª¨ë“œ ----------
        if mode == "summary":
            if doc.content:
                # ì´ ë¬¸ì„œì— í•´ë‹¹í•˜ëŠ” ì²­í¬ë“¤ ê°€ì ¸ì˜¤ê¸°
                chunks_qs = doc.chunks.all().order_by("chunk_index")
                chunk_texts = [c.content for c in chunks_qs]

                if chunk_texts:
                    try:
                        # ìš”ì•½ì— ì“¸ ëŒ€í‘œ ì²­í¬ ì„ íƒ (ì„ë² ë”© ê¸°ë°˜)
                        key_chunks = summarize_document_chunks(chunk_texts, top_k=5)
                    except Exception:
                        # summarizer ì—ëŸ¬ ì‹œ ì•ë¶€ë¶„ ëª‡ ê°œ ì²­í¬ë§Œ ì‚¬ìš©
                        key_chunks = chunk_texts[:5]

                    try:
                        # OpenAI GPTì—ê²Œ ìš”ì•½ ìš”ì²­
                        summary_text = generate_answer_with_context(
                            question=(
                                "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë³´ê³ , ëŒ€í•™ìƒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ "
                                "5ì¤„ ì´ë‚´ì˜ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜."
                            ),
                            context_chunks=key_chunks,
                        )
                    except Exception:
                        # GPT í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, ì•ë¶€ë¶„ ì˜ë¼ì„œ ë³´ì—¬ì£¼ëŠ” fallback
                        text = doc.content.strip()
                        max_len = 600
                        if len(text) > max_len:
                            summary_text = (
                                text[:max_len] + "\n...\n(ì¼ë¶€ë§Œ í‘œì‹œí•œ ìš”ì•½ì…ë‹ˆë‹¤)"
                            )
                        else:
                            summary_text = text
                else:
                    summary_text = "ì´ ë¬¸ì„œì—ëŠ” ì²­í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            else:
                summary_text = "ì´ ë¬¸ì„œì—ëŠ” ì €ì¥ëœ í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

        # ---------- (B) ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ëª¨ë“œ ----------
        elif mode == "qa":
            question = (request.POST.get("question") or "").strip()

            if question and doc.content:
                # 1) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (top_k=5, alpha=0.6ì€ rag_pipeline ìª½ì—ì„œ ê¸°ë³¸ê°’)
                search_results = hybrid_chunk_search(doc, question, top_k=5, alpha=0.6)

                if search_results:
                    # 2) ìµœìƒìœ„ ê²°ê³¼ì˜ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                    top_score = search_results[0].get("score", 0.0)
                    confidence = int(round(float(top_score) * 100))

                    # ğŸ”¹ ì„ê³„ê°’(Threshold) ì„¤ì •: ë„ˆë¬´ ë‚®ìœ¼ë©´ "ëª¨ë¥´ê² ë‹¤" ì²˜ë¦¬
                    threshold = 0.35  # í•„ìš”í•˜ë©´ 0.3~0.4 ì‚¬ì´ì—ì„œ ì¡°ì ˆí•´ ë³´ê¸°

                    if top_score < threshold:
                        # ë¬¸ì„œì™€ ì§ˆë¬¸ì´ ê±°ì˜ ì•ˆ ë§ëŠ” ê²½ìš° â†’ GPT í˜¸ì¶œ ëŒ€ì‹  ì•ˆë‚´ ë©”ì‹œì§€
                        llm_answer = (
                            "ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ë¬¸ì„œì—ì„œ ì¶©ë¶„íˆ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
                            "ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¾¸ê±°ë‚˜, ë‹¤ë¥¸ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤."
                        )
                    else:
                        # 3) ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ GPT ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                        context_chunks = [r["text"] for r in search_results]

                        # í•„ìš”í•˜ë©´ ì—¬ê¸°ì—ì„œ ë¬¸ì„œ ìš”ì•½ ë“± ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë„ ë¶™ì¼ ìˆ˜ ìˆìŒ
                        # ex) context_chunks.append("[ë¬¸ì„œ ì „ì²´ ìš”ì•½]\n" + some_summary)

                        llm_answer = generate_answer_with_context(
                            question=question,
                            context_chunks=context_chunks,
                        )

                    # 4) ì§ˆë¬¸/ë‹µë³€ ê¸°ë¡ ì €ì¥
                    QuestionHistory.objects.create(
                        document=doc,
                        question=question,
                        answer=llm_answer or "",
                        confidence=confidence,
                    )
                    # ë°©ê¸ˆ ì¶”ê°€í•œ ê¸°ë¡ í¬í•¨í•´ì„œ ë‹¤ì‹œ ì¡°íšŒ
                    history = QuestionHistory.objects.filter(document=doc).order_by(
                        "-created_at"
                    )
                else:
                    llm_answer = "ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ë‹¨ì„ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    confidence = None
            else:
                llm_answer = "ì§ˆë¬¸ì´ ë¹„ì–´ ìˆê±°ë‚˜, ë¬¸ì„œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

        # ---------- (C) ê²°ê³¼ ì´ˆê¸°í™” ëª¨ë“œ ----------
        elif mode == "reset":
            summary_text = None
            question = None
            llm_answer = None
            search_results = []
            confidence = None
            # historyëŠ” ê·¸ëŒ€ë¡œ ë‘  (ê¸°ë¡ ì‚­ì œê¹Œì§€ í•  í•„ìš”ëŠ” ì—†ìŒ)

    # 4) í…œí”Œë¦¿ì— ë„˜ê¸¸ ì»¨í…ìŠ¤íŠ¸
    context = {
        "document": doc,
        "summary_text": summary_text,
        "question": question,
        "llm_answer": llm_answer,
        "search_results": search_results,
        "confidence": confidence,
        "history": history,
    }
    return render(request, "chatbot/document_detail.html", context)

# ------------------------------------------------
# ë³„ë„ ìš”ì•½ í˜ì´ì§€ (ì„ íƒ ê¸°ëŠ¥, ì•ˆ ì“°ë©´ ì•ˆ ë“¤ì–´ê°€ë„ ë¨)
# ------------------------------------------------
def document_summary(request, pk):
    """
    ì—…ë¡œë“œëœ ë¬¸ì„œì˜ 'ìš”ì•½ í˜ì´ì§€'ë¥¼ ë³´ì—¬ì£¼ëŠ” ë·°.
    (ê°„ë‹¨ ë²„ì „ â€“ ì•ìª½ ëª‡ ê°œ ì²­í¬ë§Œ ì‚¬ìš©)
    """
    doc = get_object_or_404(UploadedDocument, pk=pk)
    chunks_qs = doc.chunks.all().order_by("chunk_index")
    chunk_texts = [c.content for c in chunks_qs]

    summary_chunks = summarize_document_chunks(chunk_texts, top_k=3) if chunk_texts else []
    summary_text = "\n\n".join(summary_chunks) if summary_chunks else ""

    context = {
        "document": doc,
        "summary_chunks": summary_chunks,
        "summary_text": summary_text,
    }
    return render(request, "chatbot/document_summary.html", context)


# ------------------------------------------------
# ëŒ€ì‹œë³´ë“œ: ì „ì²´ í†µê³„ / ìƒìœ„ ë¬¸ì„œ / ìµœê·¼ ì§ˆë¬¸
# ------------------------------------------------
def dashboard(request):
    """
    ì „ì²´ RAG í¬í„¸ì— ëŒ€í•œ ê°„ë‹¨ ëŒ€ì‹œë³´ë“œ
    - ì´ ë¬¸ì„œ ìˆ˜
    - ì´ ì§ˆë¬¸ ìˆ˜
    - ì§ˆë¬¸ì´ ë§ì€ ë¬¸ì„œ TOP5
    - ìµœê·¼ ì§ˆë¬¸ 10ê°œ
    - ì¼ìë³„ ì§ˆë¬¸ ìˆ˜
    """
    total_docs = UploadedDocument.objects.count()
    total_questions = QuestionHistory.objects.count()

    # ë¬¸ì„œë³„ ì§ˆë¬¸ ìˆ˜ TOP5
    top_docs = (
        UploadedDocument.objects
        .annotate(question_count=Count("questions"))
        .order_by("-question_count", "-uploaded_at")[:5]
    )

    # ìµœê·¼ ì§ˆë¬¸ 10ê°œ
    recent_questions = (
        QuestionHistory.objects
        .select_related("document")
        .order_by("-created_at")[:10]
    )

    # ì¼ìë³„ ì§ˆë¬¸ ìˆ˜ (ìµœê·¼ 30ì¼)
    daily_counts = (
        QuestionHistory.objects
        .annotate(day=TruncDate("created_at"))
        .values("day")
        .annotate(count=Count("id"))
        .order_by("day")
    )

    context = {
        "total_docs": total_docs,
        "total_questions": total_questions,
        "top_docs": top_docs,
        "recent_questions": recent_questions,
        "daily_counts": daily_counts,
    }
    return render(request, "chatbot/dashboard.html", context)
